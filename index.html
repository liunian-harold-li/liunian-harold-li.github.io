<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Liunian Harold Li</title>

  <!-- Bootstrap core CSS -->
  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom styles for this template -->
  <link href="css/small-business.css" rel="stylesheet">
  <link rel="stylesheet" href="css/all.css" />
  <link rel="stylesheet" href="css/academicons.css" />
  <style type="text/css">
    .content {
      margin-top: 0;
      margin-bottom: 0
    }
  </style>
  <meta http-equiv="Content-Type" content="text/html; charset=gbk">
  <meta name="google-site-verification" content="qEih9m0y-6X0QuisQYfHSxOvkW-o5Q3dfxuQ5Z4JtGA" />

</head>

<body>

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
    <div class="container">
      <a class="navbar-brand" href="">Home</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive"
        aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav ml-auto">
          <!-- <li class="nav-item active">
              <a class="nav-link" href="">Home
                <span class="sr-only">(current)</span>
              </a>
            </li> -->
          
          <li class="nav-item">
            <a class="nav-link" href="files/cv.pdf">CV</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- Page Content -->
  <div class="container">

    <!-- Heading Row -->
    <div class="row my-4">
      <div class="col-lg-4">
        <img class="img-fluid rounded img-responsive" style="width:80%" src="img/harold_cropped.jpeg" alt="">
      </div>
      <!-- /.col-lg-8 -->
      <div class="col-lg-7">
        <h1>Liunian Harold Li</h1>
        <p>Written as 李刘年</p>
        <p>Email: liunian.harold.li [at] cs [dot] ucla [dot] edu </p>
        <p class=" m-0">Hi, welcome to my homepage! I am a fifth-year Ph.D. student in the <span style="color:#4169E1"><a
              href="https://www.cs.ucla.edu/">Department of Computer Science, UCLA</a></span>. I am fortunated to work with <span
            style="color:#4169E1"><a href="http://web.cs.ucla.edu/~kwchang/">Prof. Kai-Wei Chang</a></span> focusing on problems in
          <strong> Natural Language Processing</strong> and <strong> Computer Vision</strong>.
        <p></p>
        <!-- <p>Before joining UCLA, I spent one year in <span style="color:#4169E1"><a
              href="https://engineering.virginia.edu/departments/computer-science">Department of Computer Science,
              University of Virginia</a></span>. Had a great experience in Charlottesville! </p> -->
        <!-- <a class="btn btn-primary btn-lg" href="#">Call to Action!</a> -->
        <p></p>
      </div>
      <!-- /.col-md-4 -->
    </div>
    <!-- /.row -->

    <!-- <div class=" my-4 text-left" >
            <h2>About me</h2>
            <p >Before joining UCLA, I spent one year in <span style="color:#4169E1"><a href="https://engineering.virginia.edu/departments/computer-science">Department of Computer Science, University of Virginia</a></span>. Had a great experience in Charlottesville! </p>
          <p></p>
      </div> -->


    <div class=" my-4 text-left">
      <h4>Publications (Selected) </h4> 
      <p><a href="https://scholar.google.com/citations?user=ntbhn9UAAAAJ&hl=en">[Google Scholar]</a> <a href="https://www.semanticscholar.org/author/32562635">[Semantic Scholar]</a></p>

      
      
      <ul>
        <li><b><a href="files/description_recognition.pdf">DesCo: Learning Object Recognition with Rich Language Descriptions
        </a></b>
          <p class='content'><strong>Liunian Harold Li*</strong>, Zi-Yi Dou*, Nanyun Peng, Kai-Wei Chang.
          </p>
          <p>
            <strong>Preprint. 2023</strong>. <strong><span style="color:red">
              CVPR OmniLabel 2023 Challenge Winner (1st place).
            </span></strong><a href="https://www.omnilabel.org/dataset/challenge-2023">[Link]</a>
          </p>
        </li>

        <li><b><a href="https://arxiv.org/abs/2205.11502">On the Paradox of Learning to Reason from Data
        </a></b>
          <p class='content'>Honghua Zhang, <strong>Liunian Harold Li</strong>, Tao Meng, Kai-Wei Chang, Guy Van den Broeck.
          </p>
          <p>
            <strong>IJCAI. 2023</strong>. 
          </p>
        </li>
        
          <!-- 
          GLIPv2: Unifying Localization and Vision-Language Understanding
          https://arxiv.org/abs/2206.05836
          Haotian Zhang*, Pengchuan Zhang*, Xiaowei Hu, Yen-Chun Chen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, Jianfeng Gao
          NeurIPS 2022
        -->
        <li><b><a href="https://arxiv.org/abs/2206.05836">GLIPv2: Unifying Localization and Vision-Language Understanding</a></b>
          <p class='content'> Haotian Zhang*, Pengchuan Zhang*, Xiaowei Hu, Yen-Chun Chen, <strong>Liunian Harold Li</strong>, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, Jianfeng Gao.          </p>
          <p>
            <strong>NeurIPS 2022</strong>.
          </p>
        </li>
      
        <!--
          https://arxiv.org/abs/2112.03857
          Grounded Language-Image Pre-training
          Liunian Harold Li*, Pengchuan Zhang*, Haotian Zhang*, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, Jianfeng Gao.
          CVPR 2022
        --> 
        <li><b><a href="https://arxiv.org/abs/2112.03857">Grounded Language-Image Pre-training</a></b>
          <p class='content'> <strong>Liunian Harold Li*</strong>, Pengchuan Zhang*, Haotian Zhang*, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, Jianfeng Gao.          </p>
          <p>
            <strong>CVPR 2022</strong>. 
            <strong><span style="color:red">
              Best Paper Finalist.
            </span></strong>
          </p>
        </li>


        <li><b><a href="https://arxiv.org/abs/2107.06383">How Much Can CLIP Benefit Vision-and-Language Tasks?</a></b>
          <p class='content'> Sheng Shen*, <strong>Liunian Harold Li*</strong>, Hao Tao, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, Kurt Keutzer. </p>
          <p><strong>ICLR 2022</strong>. </p>
        </li>

        <!-- 
          Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning
          https://arxiv.org/abs/2109.06860
          Da Yin, Liunian Harold Li, Ziniu Hu, Nanyun Peng, Kai-Wei Chang
          EMNLP 2021
        -->
        <li><b><a href="https://arxiv.org/abs/2109.06860">Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning</a></b>
          <p class='content'> Da Yin, <strong>Liunian Harold Li</strong>, Ziniu Hu, Nanyun Peng, Kai-Wei Chang.          </p>
          <p>
            <strong>EMNLP 2021</strong>. 
          </p>


        
        <li><b><a href="https://www.aclweb.org/anthology/2021.naacl-main.420">Unsupervised Vision-and-Language Pre-training Without Parallel Images and Captions</a></b>
          <p class='content'> <strong>Liunian Harold Li</strong>, Haoxuan You*, Zhecan Wang*, Alireza Zareian, Shih-Fu Chang, Kai-Wei Chang. </p>
          <p><strong>NAACL 2021</strong>. </p>
        </li>
        <li><b><a href="https://arxiv.org/pdf/1908.03557">VisualBERT: A Simple and Performant Baseline for Vision and Language</a></b>
          <p class='content'> <strong>Liunian Harold Li</strong>, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang. </p>
          <p><strong>Arxiv</strong>. Short Version Published as <a href="https://www.aclweb.org/anthology/2020.acl-main.469/">What Does BERT with Vision Look At?</a> at <strong>ACL 2020</strong>. </p>
        </li>
        <li><b><a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00289/43525/Efficient-Contextual-Representation-Learning-With">Efficient Contextual Representation Learning With Continuous Outputs</a></b>
          <p class='content'> <strong>Liunian Harold Li</strong>, Patrick H. Chen, Cho-Jui Hsieh, Kai-Wei Chang. </p>
          <p> <strong>TACL 2019</strong>.</p>
        </li>
       
      </ul>
    <!-- </div>
    <div class=" my-4 text-left">
      <h2>Education</h2>
      <ul>
        <li> Ph.D. (Computer Science), <span style="color:#4169E1"><a href="https://www.ucla.edu/">University of
              California, Los Angeles</a></span>. 2019.09 - Now </li>
        <li> B.S. <span style="color:#4169E1"><a href="https://english.pku.edu.cn/">Peking University</a></span>. 2015.09 - 2019.06 </li>
      </ul>
    </div> -->
    <div class=" my-4 text-left">
      <h4>Awards</h4>
      <ul>
        <li> <a href="https://research.google/outreach/phd-fellowship/recipients/">Google Ph.D. Fellowship 2023</a></li>
        <li> <a href="https://twitter.com/CVPR/status/1539772091112857600">CVPR 2022 Best Paper Finalist</a></li>
        <li> <a href="https://www.sciencehub.ucla.edu/2022-amazon-fellows/">Amazon-UCLA Fellowship 2022</a></li>
      </ul>
    </div>

    <div class=" my-4 text-left">
      <h4>Service</h4>
      <ul>
        <li>Reviewing for EMNLP, NAACL, NeurIPS, CVPR, ICCV, ICLR</li>
        <li>Honored to serve as a student co-chair for NAACL Student Research Workshop 2022</li>
        <li>Honored to serve as a workflow co-chair for AAAI 2023</li>
      </ul>
    </div>


    <!-- Content Row -->
    <!-- /.row -->

  </div>
  <!-- /.container -->

  <!-- Footer -->
  <footer class="py-1 bg-dark">
    <div class="container">
      <p class="m-0 text-center text-white">Designed via Bootstrap</p>
    </div>
    <!-- /.container -->
  </footer>

  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

</body>

</html>
